{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIND_textCNN Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchtext.vocab as torchvocab\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import random\n",
    "import snowballstemmer\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from itertools import chain\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"news.csv\",encoding='utf-8')\n",
    "dataset0 = pd.read_csv(\"dataset0.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>News_ID</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>N55528</td>\n",
       "      <td>lifestyle lifestyleroyals The Brands Queen Eli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>N19639</td>\n",
       "      <td>health weightloss 50 Worst Habits For Belly Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>N61837</td>\n",
       "      <td>news newsworld The Cost of Trump's Aid Freeze ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 News_ID                                               body\n",
       "0           0  N55528  lifestyle lifestyleroyals The Brands Queen Eli...\n",
       "1           1  N19639  health weightloss 50 Worst Habits For Belly Fa...\n",
       "2           2  N61837  news newsworld The Cost of Trump's Aid Freeze ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51282, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48616, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['length'] = news[\"body\"].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48616.000000</td>\n",
       "      <td>48616.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>25534.113707</td>\n",
       "      <td>48.945615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14788.844497</td>\n",
       "      <td>26.273286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12724.750000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25498.500000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38302.250000</td>\n",
       "      <td>76.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>51280.000000</td>\n",
       "      <td>487.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0        length\n",
       "count  48616.000000  48616.000000\n",
       "mean   25534.113707     48.945615\n",
       "std    14788.844497     26.273286\n",
       "min        0.000000      6.000000\n",
       "25%    12724.750000     29.000000\n",
       "50%    25498.500000     39.000000\n",
       "75%    38302.250000     76.000000\n",
       "max    51280.000000    487.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the article length show us, the average lenth of every article is around 48 chracter, so in the following part, we will only pick 10 artical as our input, and the rest of them will be dropped out or padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Impression ID</th>\n",
       "      <th>User ID</th>\n",
       "      <th>Impression Time</th>\n",
       "      <th>User Click History</th>\n",
       "      <th>Impression New</th>\n",
       "      <th>label</th>\n",
       "      <th>news_lst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18714</td>\n",
       "      <td>18715</td>\n",
       "      <td>U30284</td>\n",
       "      <td>11/10/2019 12:39:52 PM</td>\n",
       "      <td>N22279 N39074 N20263 N39690 N10977 N50985 N240...</td>\n",
       "      <td>N48657</td>\n",
       "      <td>1</td>\n",
       "      <td>N22279 N39074 N20263 N39690 N10977 N50985 N240...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45814</td>\n",
       "      <td>45815</td>\n",
       "      <td>U33584</td>\n",
       "      <td>11/11/2019 10:14:59 PM</td>\n",
       "      <td>N28936 N45020 N55925 N48369 N15270 N7930 N2959...</td>\n",
       "      <td>N62395</td>\n",
       "      <td>1</td>\n",
       "      <td>N28936 N45020 N55925 N48369 N15270 N7930 N2959...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2971</td>\n",
       "      <td>2972</td>\n",
       "      <td>U38515</td>\n",
       "      <td>11/13/2019 8:36:50 AM</td>\n",
       "      <td>N1150 N55846 N1569 N40962 N36739 N8419 N5696 N...</td>\n",
       "      <td>N13907</td>\n",
       "      <td>0</td>\n",
       "      <td>N1150 N55846 N1569 N40962 N36739 N8419 N5696 N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Impression ID User ID         Impression Time  \\\n",
       "0       18714          18715  U30284  11/10/2019 12:39:52 PM   \n",
       "1       45814          45815  U33584  11/11/2019 10:14:59 PM   \n",
       "2        2971           2972  U38515   11/13/2019 8:36:50 AM   \n",
       "\n",
       "                                  User Click History Impression New  label  \\\n",
       "0  N22279 N39074 N20263 N39690 N10977 N50985 N240...         N48657      1   \n",
       "1  N28936 N45020 N55925 N48369 N15270 N7930 N2959...         N62395      1   \n",
       "2  N1150 N55846 N1569 N40962 N36739 N8419 N5696 N...         N13907      0   \n",
       "\n",
       "                                            news_lst  \n",
       "0  N22279 N39074 N20263 N39690 N10977 N50985 N240...  \n",
       "1  N28936 N45020 N55925 N48369 N15270 N7930 N2959...  \n",
       "2  N1150 N55846 N1569 N40962 N36739 N8419 N5696 N...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset0.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(797054, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop null row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset0.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(780790, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = dataset0.iloc[0:10000,:]\n",
    "data0 = data0[['news_lst','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_lst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_lst\n",
       "label          \n",
       "0          7016\n",
       "1          2984"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>news_lst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.7016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  news_lst\n",
       "0      0    0.7016\n",
       "1      1    0.2984"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = data0.groupby(\"label\").count()/(7016+2984)\n",
    "a.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N62931 N15858 N62620 N53107 N29382 N40132 N63248 N19787 N38963 N24002 N56308 N16182 N64435 N36091 N6956 N56586 N1746 N47558 N10575 N44105 N53879 N57524 N28229 N30710 N63332 N5558 N32447 N51937 N25113 N33969 N5388 N1998 N54698 N15634 N57886 N42208 N55167 N57303 N6185 N65031 N62919 N6185 N16863 N49223 N63210 N1066 N36682 N32939 N37942 N40704 N46392 N18275 N32312 N59691 N56121 N25087 N53700 N2486 N37280 N50873 N1430 N62610 N36230 N63411 N21789 N822 N7426 N41616 N54024 N24092 N12349 N54024 N8952 N25134 N57318 N25677 N26544 N11323 N31801 N29867 N26778 N33975 N28229 N52396 N18656 N10964 N23718 N54179 N39749 N33742 N42620 N21984 N5283 N61664 N19594 N2018 N61664 N34113 N52551 N419 N49475 N55911 N55911 N13716 N30209 N14938 N52355 N41244 N58357 N53976 N23431 N37509 N28879 N34088 N38330 N8913 N18708\n"
     ]
    }
   ],
   "source": [
    "def conbine_search_content(news,newsid_lst):\n",
    "    history_content = []\n",
    "    try:\n",
    "        # only pick 10 article as input ... \n",
    "        for newsid in newsid_lst.split()[0:10]:\n",
    "            single = list(news[news['News_ID']== newsid]['body'])[0]\n",
    "            history_content.append(str(single))\n",
    "    except:\n",
    "        print(newsid_lst)\n",
    "    return \" \".join(history_content)\n",
    "\n",
    "train_data = []\n",
    "for newsid_lst, label in data0.values:\n",
    "    history_content = conbine_search_content(news,newsid_lst)\n",
    "    train_data.append([history_content,label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sports basketball_nba Kawhi Leonard\\'s Clippers debut leaves NBA fans wanting more Kawhi Leonard made his preseason debut with the Los Angeles Clippers on Thursday night, and the team can\\'t wait for more. sports basketball_nba Williamson wows home crowd, Pelicans beat Jazz 128-127 Zion Williamson wowed the home crowd with another impressive preseason performance, scoring 26 points and grabbing five rebounds in New Orleans\\' 128-127 win over the Utah Jazz on Friday night. nan autos autossports Refreshed Toyota 86/Subaru BRZ spied running around the Nurburgring Styling elements from both the BRZ and 86 can be seen on this car sports basketball_nba WATCH: Lonzo Ball airballs James Harden one-legged step back Lonzo Ball could make a guest appearance on Shaqtin A Fool later this week. sports basketball_nba Watch: LeBron James with epic stare down after dunk Intensity was high in the season-opening battle of Los Angeles between the Clippers and the Lakers. nan autos autosreview First Drive! 2020 Shelby GT500 Is an Apex Predator, Turning Drivers Into Track Stars Driving the 2020 Shelby GT500 on the street, dragstrip, and road course reveals an apex predator that turns any driver into a track star. foodanddrink foodnews Peter Luger\\'s says its steaks are still \\'the best you can eat\\' after zero-star review from the New York Times Pete Wells wrote that his fries were \"mealy and bland\" and that the Caesar salad\\'s croutons were \"straight out of the bag.\" tv tv-celebrity John Witherspoon Dies: Comedian & \\'Friday\\' Star Was 77 John Witherspoon, an actor-comedian who for decades made audiences laugh in television shows and films, including the hit Friday franchise, died suddenly at his home today. He was 77. \"It is with deepest sorrow that we can confirm our beloved husband and father, John Witherspoon, one of the hardest working men in show business, died sports basketball_nba Anthony Davis\\' performance against Grizzlies places him among Lakers greats George Mikan. Elgin Baylor. Wilt Chamberlain. Kareem Abdul-Jabbar. Shaquille O\\'Neal. Now, Anthony Davis. Lakers history is filled with big men who dazzled their fans and terrified opponents, and on Tuesday night Davis joined their exclusive club. They are the only Lakers to have scored at least 40 points in a game in which they also grabbed 20 rebounds. Davis scored 40 points with 20 rebounds ... movies movies-celebrity Lupita Nyong\\'o Reprises Her \\'Us\\' Character Red to Terrify Fans at Halloween Horror Nights The actress donned her iconic red jumpsuit to freak people out at Halloween Horror Nights at Universal Studios Orlando. sports basketball_nba Stephen Curry injury update: Warriors guard to undergo CT scan (hand) to see if he\\'ll need surgery Stephen Curry was going up for a layup but fell to the court and was landed on by Aron Baynes. travel travelnews Latest Australia shark attack sparks tourism concerns Tourism operators want aerial shark patrols to be introduced in Australia\\'s Whitsunday Islands as they try to stem falling visitor numbers following a spate of attacks along the Great Barrier Reef. sports basketball_nba Irving\\'s double-double lifts Nets past Harden, Rockets The Rockets\\' usual problems sent them to a 123-116 loss to the Brooklyn Nets on Friday. sports mma Jorge Masvidal says he\\'s \\'dead serious\\' about wanting to box against Canelo Alvarez The BMF champ thinks he has a legitimate chance to knock out Canelo in a boxing fight video animals Giant octopus latches onto scuba diver\\'s camera A scuba diver came face-to-face with a giant Pacific octopus near Seattle when it latched onto the diver\\'s camera and wouldn\\'t let go. sports basketball_nba Report: Many NBA executives believe Andre Iguodala will be traded to Lakers The notion that Iguodala is likely to land with the Lakers is so strong that there is a belief in league circles that it could dissuade other teams from engaging in trade dialogue with Memphis. finance finance-companies Why US miners are still digging new mines as coal prices plunge As coal prices tumble and bankruptcies rise, a few U.S. miners are still pushing ahead with plans to expand.',\n",
       " 0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0cbcb1423622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"comment\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_data,columns=[\"comment\",\"label\"])\n",
    "train_df.groupby([\"label\"]).count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting words\n",
    "Next step is the word splitting, here only do very simple word splitting, that is, by space. Of course following some traditional cleaning method will work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74607 unique words in the data set\n"
     ]
    }
   ],
   "source": [
    "# \"vocab\" would be all the words appear in the train_data_set\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok.lower() for tok in text.split(' ')]\n",
    "\n",
    "train_tokenized = []\n",
    "for review, score in train_data:\n",
    "    train_tokenized.append(tokenizer(review))\n",
    "\n",
    "# test_tokenized = []\n",
    "# for review, score in test_data:\n",
    "#     test_tokenized.append(tokenizer(review))\n",
    "\n",
    "# itertools.chain, Combine N list to form a new single list.\n",
    "vocab = set(chain(*train_tokenized))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"{} unique words in the data set\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word embedding\n",
    "\n",
    "Since this dataset is very small, there is a risk of overfitting if we do word embedding with this dataset, and the model will not be generalized, so we pass in a pre-trained word embedding. The data used glove-twitter-100 with 387MB and 100 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model and return as object ready for use\n",
    "dimension = 100\n",
    "model_glove_twitter = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44104  ,  0.1385   , -0.66489  , -0.044309 ,  0.44579  ,\n",
       "        0.027886 , -0.30068  , -0.13851  ,  0.44771  ,  0.60006  ,\n",
       "        0.12149  , -0.69262  , -3.5289   , -0.5495   , -0.98539  ,\n",
       "        0.54288  , -0.17355  , -0.73415  , -0.46325  , -0.68942  ,\n",
       "       -0.29029  , -0.20679  , -1.0008   , -0.010779 , -0.52833  ,\n",
       "       -2.9566   ,  0.45207  , -0.65441  ,  0.10636  ,  0.15182  ,\n",
       "       -0.71115  ,  0.17282  , -0.16225  , -0.96776  ,  0.64226  ,\n",
       "       -0.029472 ,  0.5799   ,  0.18865  , -0.022253 , -0.61489  ,\n",
       "       -1.1467   ,  0.39476  , -0.2715   , -0.024786 ,  0.32542  ,\n",
       "       -0.14626  , -0.13835  ,  0.44469  , -0.72034  ,  0.0059288,\n",
       "        0.069213 , -0.042943 , -0.32557  , -0.4062   , -0.023224 ,\n",
       "        0.74154  , -1.5501   , -0.012535 , -0.020187 , -0.31557  ,\n",
       "        0.036324 , -0.56278  ,  0.072553 , -0.02491  , -0.53492  ,\n",
       "        0.49579  ,  0.24916  ,  0.92282  , -0.20315  ,  0.27591  ,\n",
       "       -0.71818  ,  0.39903  , -0.078875 , -0.38303  , -0.84732  ,\n",
       "        0.80215  , -0.59038  , -0.30123  ,  0.034802 ,  0.34928  ,\n",
       "        0.36331  ,  0.2053   ,  0.44075  , -0.15293  , -0.16563  ,\n",
       "       -0.21373  , -0.3372   ,  0.10873  ,  0.23909  , -0.68149  ,\n",
       "       -0.33458  , -0.2037   , -0.14106  , -0.1777   ,  0.58009  ,\n",
       "       -0.38089  ,  0.20533  ,  0.17301  ,  0.079456 , -0.25333  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter[\"twitter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a word to index lexicon \n",
    "\n",
    "The purpose of the definition is to connect the pre-trained weight with our lexicon.    \n",
    "In addition, we define an unknown word, which means that any word that does not appear in the training set is called unknown and the word vector is defined as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unknow word index=0, the other word start from index=1\n",
    "word_to_idx = {word: i+1 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<unk>'] = 0\n",
    "\n",
    "idx_to_word = {i+1: word for i, word in enumerate(vocab)}\n",
    "idx_to_word[0] = '<unk>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding\n",
    "\n",
    "In order to solve the problem of inconsistent comment length, we take all news content with 500 words limitations, taking the first 500 if they exceed, and making up 0 if they fall short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_samples(tokenized_samples, vocab):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in word_to_idx:\n",
    "                feature.append(word_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def pad_samples(features, maxlen=500, PAD=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) >= maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            while(len(padded_feature) < maxlen):\n",
    "                padded_feature.append(PAD)\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"vocab\" would be all the words appear in the train_data_set\n",
    "\n",
    "# \"train_tokenized\" would be [[tokenized word in comment1],[tokenized word in comment2],...] \n",
    "# (i.g. train_tokenized[0:2])\n",
    "\n",
    "# \"train_data\" would be [[comment1,label1],[comment2,label2]...]\n",
    "# (i.g. train_data[0:2])\n",
    "\n",
    "train_features = torch.tensor(pad_samples(encode_samples(train_tokenized, vocab)))\n",
    "train_labels = torch.tensor([score for _, score in train_data])\n",
    "# test_features = torch.tensor(pad_samples(encode_samples(test_tokenized, vocab)))\n",
    "# test_labels = torch.tensor([score for _, score in test_data])\n",
    "\n",
    "\n",
    "# TODO: change the test set\n",
    "test_features = train_features.clone()\n",
    "test_labels = train_labels.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features = torch.Size([10000, 500])\n",
      "train_labels = torch.Size([10000])\n",
      "test_features = torch.Size([10000, 500])\n",
      "test_labels = torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(\"train_features = {}\".format(train_features.shape))\n",
    "print(\"train_labels = {}\".format(train_labels.shape))\n",
    "print(\"test_features = {}\".format(test_features.shape))\n",
    "print(\"test_labels = {}\".format(test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-79ede583fd2a>:19: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  weight[index, :] = torch.from_numpy(model_glove_twitter.get_vector(word))\n"
     ]
    }
   ],
   "source": [
    "embed_size = 100\n",
    "\n",
    "# create the pretrained - word Embedding\n",
    "weight = torch.zeros(vocab_size+1, embed_size)\n",
    "\n",
    "for i in range(len(model_glove_twitter.index_to_key)):\n",
    "    try:\n",
    "        # use this statement to capture the case that glove_word not in our word_base\n",
    "        # if glove_word is in word_to_idx, get the word index\n",
    "        # else raise Error\n",
    "        glove_word = model_glove_twitter.index_to_key[i]\n",
    "        index = word_to_idx[glove_word]\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    glove_word = model_glove_twitter.index_to_key[i]\n",
    "    index = word_to_idx[glove_word]\n",
    "    word = idx_to_word[index]\n",
    "    weight[index, :] = torch.from_numpy(model_glove_twitter.get_vector(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-train model have 1193514 key words\n"
     ]
    }
   ],
   "source": [
    "print(\"pre-train model have {} key words\".format( len(model_glove_twitter.index_to_key) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twitter'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter.index_to_key[218]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter.key_to_index[\"twitter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49871"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx['twitter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_word = twitter\n",
      "index = 49871\n",
      "word = twitter\n",
      "output vector = (100,)\n",
      "[ 0.44104    0.1385    -0.66489   -0.044309   0.44579    0.027886\n",
      " -0.30068   -0.13851    0.44771    0.60006    0.12149   -0.69262\n",
      " -3.5289    -0.5495    -0.98539    0.54288   -0.17355   -0.73415\n",
      " -0.46325   -0.68942   -0.29029   -0.20679   -1.0008    -0.010779\n",
      " -0.52833   -2.9566     0.45207   -0.65441    0.10636    0.15182\n",
      " -0.71115    0.17282   -0.16225   -0.96776    0.64226   -0.029472\n",
      "  0.5799     0.18865   -0.022253  -0.61489   -1.1467     0.39476\n",
      " -0.2715    -0.024786   0.32542   -0.14626   -0.13835    0.44469\n",
      " -0.72034    0.0059288  0.069213  -0.042943  -0.32557   -0.4062\n",
      " -0.023224   0.74154   -1.5501    -0.012535  -0.020187  -0.31557\n",
      "  0.036324  -0.56278    0.072553  -0.02491   -0.53492    0.49579\n",
      "  0.24916    0.92282   -0.20315    0.27591   -0.71818    0.39903\n",
      " -0.078875  -0.38303   -0.84732    0.80215   -0.59038   -0.30123\n",
      "  0.034802   0.34928    0.36331    0.2053     0.44075   -0.15293\n",
      " -0.16563   -0.21373   -0.3372     0.10873    0.23909   -0.68149\n",
      " -0.33458   -0.2037    -0.14106   -0.1777     0.58009   -0.38089\n",
      "  0.20533    0.17301    0.079456  -0.25333  ]\n",
      "pretrained-embedding(weight.shape) = torch.Size([74608, 100])\n"
     ]
    }
   ],
   "source": [
    "i = 218\n",
    "glove_word = model_glove_twitter.index_to_key[i]\n",
    "print(f\"glove_word = {glove_word}\")\n",
    "index = word_to_idx[glove_word]\n",
    "print(f\"index = {index}\")\n",
    "word = idx_to_word[index]\n",
    "print(f\"word = {word}\")\n",
    "vector = model_glove_twitter.get_vector(word)\n",
    "print(f\"output vector = {vector.shape}\")\n",
    "print(vector)\n",
    "\n",
    "print(f\"pretrained-embedding(weight.shape) = {weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, seq_len, labels, weight, **kwargs):\n",
    "        super(textCNN, self).__init__(**kwargs)\n",
    "        self.labels = labels\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.conv1 = nn.Conv2d(1, 1, (3, embed_size))\n",
    "        self.conv2 = nn.Conv2d(1, 1, (4, embed_size))\n",
    "        self.conv3 = nn.Conv2d(1, 1, (5, embed_size))\n",
    "        self.pool1 = nn.MaxPool2d((seq_len - 3 + 1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((seq_len - 4 + 1, 1))\n",
    "        self.pool3 = nn.MaxPool2d((seq_len - 5 + 1, 1))\n",
    "        self.linear = nn.Linear(3, labels)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs).view(inputs.shape[0], 1, inputs.shape[1], -1)\n",
    "        x1 = F.relu(self.conv1(inputs))\n",
    "        x2 = F.relu(self.conv2(inputs))\n",
    "        x3 = F.relu(self.conv3(inputs))\n",
    "        \n",
    "        x1 = self.dropout(x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        x3 = self.dropout(x3)\n",
    "\n",
    "        x1 = self.pool1(x1)\n",
    "        x2 = self.pool2(x2)\n",
    "        x3 = self.pool3(x3)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), -1)\n",
    "        x = x.view(inputs.shape[0], 1, -1)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1, self.labels)\n",
    "\n",
    "        return(x)\n",
    "    \n",
    "num_epochs = 10\n",
    "num_hiddens = 100\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 2\n",
    "lr = 0.8\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = False\n",
    "\n",
    "\n",
    "net = textCNN(vocab_size=(vocab_size+1), embed_size=embed_size,\n",
    "              seq_len=500, labels=labels, weight=weight)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textCNN(\n",
       "  (embedding): Embedding(74608, 100)\n",
       "  (conv1): Conv2d(1, 1, kernel_size=(3, 100), stride=(1, 1))\n",
       "  (conv2): Conv2d(1, 1, kernel_size=(4, 100), stride=(1, 1))\n",
       "  (conv3): Conv2d(1, 1, kernel_size=(5, 100), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=(498, 1), stride=(498, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool2): MaxPool2d(kernel_size=(497, 1), stride=(497, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool3): MaxPool2d(kernel_size=(496, 1), stride=(496, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear): Linear(in_features=3, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_set = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_lst = []\n",
    "train_acc_lst = []\n",
    "test_loss_lst = []\n",
    "test_acc_lst = [] \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss, test_losses = 0, 0\n",
    "    train_acc, test_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "    for feature, label in train_iter:\n",
    "        n += 1\n",
    "        net.train()\n",
    "        net.zero_grad()\n",
    "        feature = Variable(feature)\n",
    "        label = Variable(label)\n",
    "        score = net(feature)\n",
    "        loss = loss_function(score, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                 dim=1), label.cpu())\n",
    "        train_loss += loss\n",
    "    with torch.no_grad():\n",
    "        for test_feature, test_label in test_iter:\n",
    "            m += 1\n",
    "            net.eval()\n",
    "            test_feature = test_feature\n",
    "            test_label = test_label\n",
    "            test_score = net(test_feature)\n",
    "            test_loss = loss_function(test_score, test_label)\n",
    "            test_acc += accuracy_score(torch.argmax(test_score.cpu().data,\n",
    "                                                    dim=1), test_label.cpu())\n",
    "            test_losses += test_loss\n",
    "    end = time.time()\n",
    "    runtime = end - start\n",
    "    print('epoch: %d, train loss: %.4f, train acc: %.2f, test loss: %.4f, test acc: %.2f, time: %.2f' %\n",
    "          (epoch, train_loss.data / n, train_acc / n, test_losses.data / m, test_acc / m, runtime))\n",
    "\n",
    "    train_loss_lst.append(train_loss.data / n)\n",
    "    train_acc_lst.append(train_acc / n)\n",
    "    test_loss_lst.append(test_losses.data / m)\n",
    "    test_acc_lst.append(test_acc / m)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
